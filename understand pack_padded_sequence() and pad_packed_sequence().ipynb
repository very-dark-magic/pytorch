{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import LongTensor\n",
    "from torch.nn import Embedding, LSTM\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "# https://github.com/HarshTrivedi/packing-unpacking-pytorch-minimal-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = [\"long_str\",\n",
    "       \"tiny\",\n",
    "       \"medium\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '_', 'd', 'e', 'g', 'i', 'l', 'm', 'n', 'o', 'r', 's', 't', 'u', 'y']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = [\"<pad>\"] + sorted(set([c for seq in seqs for c in seq]))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 9, 8, 4, 1, 11, 12, 10], [12, 5, 8, 14], [7, 3, 2, 5, 13, 7]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_seqs = [[vocab.index(tok) for tok in seq] for seq in seqs]\n",
    "vectorized_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(len(vocab), 4)\n",
    "lstm = LSTM(input_size=4, hidden_size=5, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 4, 6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lengths = LongTensor(list(map(len, vectorized_seqs)))\n",
    "seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
    "seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  9,  8,  4,  1, 11, 12, 10],\n",
       "        [12,  5,  8, 14,  0,  0,  0,  0],\n",
       "        [ 7,  3,  2,  5, 13,  7,  0,  0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "    seq_tensor[idx, :seq_len] = LongTensor(seq)\n",
    "    \n",
    "seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3746,  1.7937,  0.4702, -0.1266],\n",
       "         [ 0.0530,  0.5602, -1.2176, -1.3647],\n",
       "         [ 1.1737, -0.8779, -1.5115,  2.4027],\n",
       "         [-1.5345, -0.2037, -0.3288,  0.6264],\n",
       "         [-0.0025,  0.9873, -0.0149,  0.9598],\n",
       "         [-0.6353, -1.9604, -0.6830,  0.6624],\n",
       "         [ 0.8536,  0.5150,  0.8952,  0.3396],\n",
       "         [-1.2177, -1.5947,  0.2392, -1.0436]],\n",
       "\n",
       "        [[ 0.8536,  0.5150,  0.8952,  0.3396],\n",
       "         [-2.3059, -0.2358,  0.9411, -0.3228],\n",
       "         [ 1.1737, -0.8779, -1.5115,  2.4027],\n",
       "         [ 0.3702,  0.7784,  1.0046, -1.4837],\n",
       "         [ 0.2173,  0.5291,  0.5392, -0.5921],\n",
       "         [ 0.2173,  0.5291,  0.5392, -0.5921],\n",
       "         [ 0.2173,  0.5291,  0.5392, -0.5921],\n",
       "         [ 0.2173,  0.5291,  0.5392, -0.5921]],\n",
       "\n",
       "        [[ 0.0230, -2.0035, -0.2308,  1.2062],\n",
       "         [ 0.1585,  0.5577,  0.6678, -1.4421],\n",
       "         [ 1.2074, -1.1181,  0.9672, -0.6232],\n",
       "         [-2.3059, -0.2358,  0.9411, -0.3228],\n",
       "         [ 1.4578, -0.1260, -0.9923,  0.0044],\n",
       "         [ 0.0230, -2.0035, -0.2308,  1.2062],\n",
       "         [ 0.2173,  0.5291,  0.5392, -0.5921],\n",
       "         [ 0.2173,  0.5291,  0.5392, -0.5921]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_seq_tensor = embed(seq_tensor)\n",
    "embedded_seq_tensor # batch_size * max_seq_len * embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3746,  1.7937,  0.4702, -0.1266],\n",
       "        [ 0.0230, -2.0035, -0.2308,  1.2062],\n",
       "        [ 0.8536,  0.5150,  0.8952,  0.3396],\n",
       "        [ 0.0530,  0.5602, -1.2176, -1.3647],\n",
       "        [ 0.1585,  0.5577,  0.6678, -1.4421],\n",
       "        [-2.3059, -0.2358,  0.9411, -0.3228],\n",
       "        [ 1.1737, -0.8779, -1.5115,  2.4027],\n",
       "        [ 1.2074, -1.1181,  0.9672, -0.6232],\n",
       "        [ 1.1737, -0.8779, -1.5115,  2.4027],\n",
       "        [-1.5345, -0.2037, -0.3288,  0.6264],\n",
       "        [-2.3059, -0.2358,  0.9411, -0.3228],\n",
       "        [ 0.3702,  0.7784,  1.0046, -1.4837],\n",
       "        [-0.0025,  0.9873, -0.0149,  0.9598],\n",
       "        [ 1.4578, -0.1260, -0.9923,  0.0044],\n",
       "        [-0.6353, -1.9604, -0.6830,  0.6624],\n",
       "        [ 0.0230, -2.0035, -0.2308,  1.2062],\n",
       "        [ 0.8536,  0.5150,  0.8952,  0.3396],\n",
       "        [-1.2177, -1.5947,  0.2392, -1.0436]],\n",
       "       grad_fn=<PackPaddedSequenceBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pack_padded_sequence with embedded instances and sequence lengths\n",
    "packed_input = pack_padded_sequence(embedded_seq_tensor, seq_lengths.cpu().numpy(), \n",
    "                                    batch_first=True, enforce_sorted=False)\n",
    "packed_input.data # sum_batch_seq_len * embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0803, -0.0008,  0.1106, -0.0464,  0.0202],\n",
       "        [ 0.1156, -0.0932,  0.1572, -0.0889,  0.2454],\n",
       "        [ 0.1609, -0.0393,  0.1558, -0.0386,  0.0827],\n",
       "        [ 0.0642,  0.1866,  0.0519, -0.0030,  0.0769],\n",
       "        [ 0.2906,  0.0038,  0.0832,  0.0241,  0.1964],\n",
       "        [ 0.2751,  0.0974,  0.2204, -0.0334,  0.0410],\n",
       "        [-0.0676, -0.1134,  0.2056, -0.2809,  0.1253],\n",
       "        [ 0.3752, -0.0331,  0.1202,  0.0687,  0.3952],\n",
       "        [ 0.0017, -0.1094,  0.2199, -0.2948,  0.1525],\n",
       "        [-0.1339, -0.0508,  0.2306, -0.3195,  0.0483],\n",
       "        [ 0.4770,  0.0974,  0.1844,  0.0531,  0.1096],\n",
       "        [ 0.2375, -0.0085,  0.1454, -0.0260,  0.1530],\n",
       "        [-0.1744, -0.0583,  0.2565, -0.2723,  0.0005],\n",
       "        [ 0.2957, -0.0123,  0.1131, -0.0499,  0.2820],\n",
       "        [-0.0698, -0.0815,  0.2116, -0.1960,  0.2247],\n",
       "        [ 0.2971, -0.0797,  0.2064, -0.1237,  0.3482],\n",
       "        [ 0.1297, -0.0813,  0.2466, -0.0871,  0.2772],\n",
       "        [ 0.3546,  0.0675,  0.1533,  0.0024,  0.2131]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_output, (ht, ct) = lstm(packed_input)\n",
    "packed_output.data # sum_batch_seq_len * embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 3, 3, 2, 2, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_output.batch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0803, -0.0008,  0.1106, -0.0464,  0.0202],\n",
       "         [ 0.0642,  0.1866,  0.0519, -0.0030,  0.0769],\n",
       "         [-0.0676, -0.1134,  0.2056, -0.2809,  0.1253],\n",
       "         [-0.1339, -0.0508,  0.2306, -0.3195,  0.0483],\n",
       "         [-0.1744, -0.0583,  0.2565, -0.2723,  0.0005],\n",
       "         [-0.0698, -0.0815,  0.2116, -0.1960,  0.2247],\n",
       "         [ 0.1297, -0.0813,  0.2466, -0.0871,  0.2772],\n",
       "         [ 0.3546,  0.0675,  0.1533,  0.0024,  0.2131]],\n",
       "\n",
       "        [[ 0.1609, -0.0393,  0.1558, -0.0386,  0.0827],\n",
       "         [ 0.2751,  0.0974,  0.2204, -0.0334,  0.0410],\n",
       "         [ 0.0017, -0.1094,  0.2199, -0.2948,  0.1525],\n",
       "         [ 0.2375, -0.0085,  0.1454, -0.0260,  0.1530],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.1156, -0.0932,  0.1572, -0.0889,  0.2454],\n",
       "         [ 0.2906,  0.0038,  0.0832,  0.0241,  0.1964],\n",
       "         [ 0.3752, -0.0331,  0.1202,  0.0687,  0.3952],\n",
       "         [ 0.4770,  0.0974,  0.1844,  0.0531,  0.1096],\n",
       "         [ 0.2957, -0.0123,  0.1131, -0.0499,  0.2820],\n",
       "         [ 0.2971, -0.0797,  0.2064, -0.1237,  0.3482],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
       "       grad_fn=<IndexSelectBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unpack\n",
    "output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3546,  0.0675,  0.1533,  0.0024,  0.2131],\n",
       "         [ 0.2375, -0.0085,  0.1454, -0.0260,  0.1530],\n",
       "         [ 0.2971, -0.0797,  0.2064, -0.1237,  0.3482]]],\n",
       "       grad_fn=<IndexSelectBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch_size X max_seq_len X embedding_dim) --> Sort by seqlen ---> (batch_size X max_seq_len X embedding_dim)\n",
    "# (batch_size X max_seq_len X embedding_dim) --->      Pack     ---> (batch_sum_seq_len X embedding_dim)\n",
    "# (batch_sum_seq_len X embedding_dim)        --->      LSTM     ---> (batch_sum_seq_len X hidden_dim)\n",
    "# (batch_sum_seq_len X hidden_dim)           --->    UnPack     ---> (batch_size X max_seq_len X hidden_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
